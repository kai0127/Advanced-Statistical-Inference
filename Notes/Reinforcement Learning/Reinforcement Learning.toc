\contentsline {section}{\numberline {1}Topic: Markov Decision Processes}{1}{section.1}%
\contentsline {subsection}{\numberline {1.1}Introduction}{1}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Goals and Rewards}{2}{subsection.1.2}%
\contentsline {subsection}{\numberline {1.3}Episodes}{2}{subsection.1.3}%
\contentsline {subsection}{\numberline {1.4}Value Functions}{3}{subsection.1.4}%
\contentsline {subsection}{\numberline {1.5}Optimal Policies}{4}{subsection.1.5}%
\contentsline {subsection}{\numberline {1.6}The policy improvement theorem}{5}{subsection.1.6}%
\contentsline {subsection}{\numberline {1.7}Value Iteration}{5}{subsection.1.7}%
\contentsline {subsection}{\numberline {1.8}Generalized policy iteration}{6}{subsection.1.8}%
\contentsline {subsection}{\numberline {1.9}Monte Carlo Methods}{6}{subsection.1.9}%
\contentsline {subsection}{\numberline {1.10}Policy Gradient Methods }{8}{subsection.1.10}%
\contentsline {subsection}{\numberline {1.11}On-policy value function estimation}{8}{subsection.1.11}%
\contentsline {subsection}{\numberline {1.12}Stochastic-gradient and Semi-gradient Methods}{9}{subsection.1.12}%
\contentsline {subsection}{\numberline {1.13}Semi-gradient control}{10}{subsection.1.13}%
\contentsline {subsection}{\numberline {1.14}Policy gradient}{10}{subsection.1.14}%
