\documentclass[11pt]{article}

\usepackage{amsmath,amsthm,amssymb}

%%%%% Matrix stretcher
% use it as:
%\begin{pmatrix}[1.5]
% ...
\makeatletter
\renewcommand*\env@matrix[1][\arraystretch]{%
  \edef\arraystretch{#1}%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols c}}
\makeatother
%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand\extrafootertext[1]{%
    \bgroup
    \renewcommand\thefootnote{\fnsymbol{footnote}}%
    \renewcommand\thempfootnote{\fnsymbol{mpfootnote}}%
    \footnotetext[0]{#1}%
    \egroup
}


%%%%%%%%%%%%% Colors %%%%%%%%%%%%%
\usepackage[dvipsnames]{xcolor}

\definecolor{C0}{HTML}{1d1d1d}
\definecolor{C1}{HTML}{1e3668}
\definecolor{C2}{HTML}{199d8b}
\definecolor{C3}{HTML}{d52f4c}
\definecolor{C4}{HTML}{5ab2d6}
\definecolor{C5}{HTML}{ffb268}
\definecolor{C6}{HTML}{ff7300} % for commenting - {fire orange}dd571c
\definecolor{C7}{HTML}{777b7e} % for remarks - {steel grey}
\color{C0}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%% Fonts %%%%%%%%%%%%% 
%\usepackage{fontspec}
\usepackage[no-math]{fontspec} % for text

\emergencystretch=8pt
\hyphenpenalty=1000 % default 50
\tolerance=800      % default 200
%\righthyphenmin=4
%\lefthyphenmin=4

%%% Text Font: Vollkorn + Math Font: Latin Modern (default) %%%
\setmainfont{Vollkorn}[
UprightFont = Vollkorn-Regular,
ItalicFont =Vollkorn-Italic, 
BoldItalicFont={Vollkorn-BoldItalic},
BoldFont = Vollkorn-Bold,
RawFeature=+lnum,
WordSpace=1.7,
] 

%%% We need this for math font packages other than latin modern %%%
% \usepackage{unicode-math}        % for math

%%% Text Font: Palatino + Math Font: Asana-Math %%%
%\setmainfont{Palatino}[
%BoldFont = Palatino-Bold,
%ItalicFont = Palatino-Italic,
%BoldItalicFont={Palatino-BoldItalic},
%RawFeature=+lnum,
%WordSpace=1.7,
%]
%\setmathfont{asana-math}

%%% Text Font: Arno Pro + Math Font: Minion Pro %%%
%\setmainfont{Arno Pro}[
%UprightFont = *-Regular,
%ItalicFont = Vollkorn-Italic, 
%BoldItalicFont={*-BoldItalic},
%BoldFont = *-Bold,
%RawFeature=+lnum,
%WordSpace=1.7,
%Scale= 1.1
%] 
% Minion Pro is too expensive

%%% Math Fonts %%%
%\setmathfont{Vollkorn}
%\setmathfont{Latin Modern Math}
%\setmathfont{TeX Gyre Pagella Math}
%\setmathfont{TeX Gyre Termes Math}
%\setmathfont{TeX Gyre DejaVu Math}
%\setmathfont[Scale=MatchLowercase]{DejaVu Math TeX Gyre}
%\setmathfont{XITS Math}
%\setmathfont{Libertinus Math}
%\setmathfont[Scale=MatchUppercase]{Asana Math}
%\setmathfont{STIX Two Math}

%\usepackage{kpfonts-otf}
%\setmathfont{KpMath-Regular.otf}[version=regular]
%\setmathfont{KpMath-Bold.otf}[version=bold]
%\setmathfont{KpMath-Semibold.otf}[version=semibold]
%\setmathfont{KpMath-Sans.otf}[version=sans]
%\setmathfont{KpMath-Light.otf}[version=light]


%%% CJK Fonts %%%
\usepackage[scale=.78]{luatexja-fontspec}
\setmainjfont{BabelStone Han}[AutoFakeBold]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% This package simplifies the insertion of external multi-page PDF or PS documents.
\usepackage{pdfpages}

% cref
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=C4,
    filecolor=magenta,      
    urlcolor=cyan,
    }

\usepackage[nameinlink,noabbrev,capitalize]{cleveref}
% \crefname{ineq}{}{}
% \crefname{equation}{}{}
% \creflabelformat{ineq}{#2{\textup{(1)}}#3}
% \creflabelformat{equation}{#2\textup{(#1)}#3}

%%%%%%%%%%%%% Environments %%%%%%%%%%%%%%%%
%amsthm has three separate predefined styles:	
%
%\theoremstyle{plain} is the default. it sets the text in italic and adds extra space above and below the \newtheorems listed below it in the input. it is recommended for theorems, corollaries, lemmas, propositions, conjectures, criteria, and (possibly; depends on the subject area) algorithms.
%
%\theoremstyle{definition} adds extra space above and below, but sets the text in roman. it is recommended for definitions, conditions, problems, and examples; i've alse seen it used for exercises.
%
%\theoremstyle{remark} is set in roman, with no additional space above or below. it is recommended for remarks, notes, notation, claims, summaries, acknowledgments, cases, and conclusions.

%%%  theorem-like environment %%%
\theoremstyle{plain} % default theorem style
\newtheorem{theorem}{Theorem}[section]
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{property}[theorem]{Property}

\newtheorem{definition}[theorem]{Definition}

%%% definition-like environment %%%
%\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{problem}[theorem]{Problem}


%%% framed package is great %%%
\usepackage{framed}
\newenvironment{solution}
{\color{C2}\normalfont\begin{framed}\begingroup\textbf{Solution:} }
  {\endgroup\end{framed}}
  
\newenvironment{topic}
{\color{C2}\normalfont\begin{framed}\begingroup }
  {\endgroup\end{framed}}
  
\newtheoremstyle{remark}% name of the style to be used
  {}% measure of space to leave above the theorem. E.g.: 3pt
  {}% measure of space to leave below the theorem. E.g.: 3pt
  {\color{C3}}% name of font to use in the body of the theorem
  {}% measure of space to indent
  {\color{C3}\bfseries}% name of head font
  {.}% punctuation between head and body
  { }% space after theorem head; " " = normal interword space
  {}
\theoremstyle{remark}
\newtheorem{remarkx}[theorem]{Remark}
\newenvironment{remark}
  {\pushQED{\qed}\renewcommand{\qedsymbol}{$\triangle$}\remarkx}
  {\popQED\endremarkx}
  
\newenvironment{point}
  {\O~~}
  {}

\usepackage{thmtools}
\usepackage{thm-restate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% This package is for the long equal sign \xlongequal{}
\usepackage{extarrows}

%%%%%%%%%%%% Algorithms %%%%%%%%%%%%
\usepackage{etoolbox} 
\usepackage{setspace}
\usepackage{algorithm}
\AtBeginEnvironment{algorithmic}{\onehalfspacing}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}

\algrenewcommand\algorithmicindent{1.0em}
\let\Algorithm\algorithm
\renewcommand\algorithm[1][]{\Algorithm[#1]}%\fontsize{11}{16}\selectfont}

\newenvironment{labelalgorithm}[4][t]{%
\begin{algorithm}[#1]
%\newcommand{\thealgorithmlabel}{#2}
\newcommand{\thealgorithmname}{#3}
%\newcommand{\thealgorithmcap}{#4}
\customlabel{alg:name:#2}{\textproc{#3}}
%\customlabel{alg:cap:#2}{#4}
\caption{#4}\label{alg:#2}
}{\end{algorithm}}


\makeatletter
\newcommand{\customlabel}[2]{%
   \protected@write \@auxout {}{\string \newlabel {#1}{{#2}{\thepage}{#2}{#1}{}} }%
   \hypertarget{#1}{}%
}
\makeatother


%\algdef{SE}[FUNCTION]{Procedure}{EndProcedure}%
%   [2]{\algorithmicclass\ \textproc{#1}\ifthenelse{\equal{#2}{}}{}{$($#2$)$}}%
%   {\algorithmicend\ \algorithmicclass}%

\algnewcommand\algorithmicclass{\textbf{class}}
\algdef{SE}[FUNCTION]{Class}{EndClass}%
   [2]{\algorithmicclass\ \textproc{#1}\ifthenelse{\equal{#2}{}}{}{$($#2$)$}}%
   {\algorithmicend\ \algorithmicclass}%

% Tells algorithmicx not to print an empty line if `noend' is set 
\makeatletter
\ifthenelse{\equal{\ALG@noend}{t}}%
  {\algtext*{EndClass}}
  {}%
\makeatother
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Page Formatting
\usepackage[
    paper=a3paper,
    inner=22mm,         % Inner margin
    outer=22mm,         % Outer margin
    bindingoffset=0mm, % Binding offset
    top=28mm,           % Top margin
    bottom=22mm,        % Bottom margin
    %showframe,         % show how the type block is set on the page
]{geometry}

\setlength{\parindent}{0em}
\setlength{\parskip}{.7em}


\usepackage{tikz}
\usepackage{graphicx}
\usepackage{subcaption} % for subplots
\usepackage{multicol}   % for multicolumns
\usepackage{wrapfig}    % for inserting figures in multicolumns
\usepackage{enumitem}
\setlist{topsep=0pt}

\usepackage{bm}

\usepackage[font=scriptsize,labelfont=bf]{caption}
\usepackage{listings}
\lstset{basicstyle=\ttfamily,breaklines=true}
% \setlength{\parskip}{1em}
% \setlength{\parindent}{0em}
\usepackage{dsfont}
\newcommand{\bOne}{\mathds{1}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\VV}{\mathbb{V}}
\newcommand{\CoV}{\operatorname{Co\mathbb{V}}}

% header
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[L]{\small   \bfseries Notes}
\fancyhead[C]{\small   \bfseries Fall 2023}
\fancyhead[R]{\small   \bfseries Zhou}


\begin{document}

\begin{center}
  \text{\Large{Model Selection
    }}

  {\text{Kaiwen Zhou}}
\end{center}
\vspace{2em}

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Two levels of inference}
Two levels of inference can often be distinguished in the process of data
modeling.

At the first level of inference, we assume that a particular model is true,
and we fit that model to the data, i.e., we infer what values its free
parameters should plausibly take, given the data. The results of this
inference are often summarized by the most probable parameter values, and
error bars on those parameters. This analysis is repeated for each model.

The second level of inference is the task of model comparison, in which the goal
is to compare the models in the light of the data, and assign some sort of
preference or ranking to the alternatives.

Model comparison is a difficult task because it is not possible simply to choose
the model that fits the data best: more complex models can always fit the data
better, so the maximum likelihood model choice would lead us inevitably to
implausible, over-parameterized models, which generalize poorly. Perhaps
Bayes' theorem can help with this somewhat-difficult task.

Let us write down Bayes' theorem for the two levels of inference described
above. Each model $m_{i}$ is assumed to have a vector of parameters $\theta$.
A model is defined by a collection of probability distributions: a `prior'
distribution
$$
  p\left(\theta \mid m_{i}\right)
$$
which states what values the model's parameters might be expected to take; and a
set of conditional distributions,
$$
  p\left(D \mid \theta, m_{i}\right),
$$
one for each value of $\theta$, defining the
predictions that the model makes about the data $D$.

\subsection{The first level of inference}
At the first level of inference, we focus on one model, the $i$-th
model, and we infer what the model's parameters $\theta$ might be, given the
data $D$.

Using Bayes' theorem, the posterior probability of the parameters
$\theta$ is:
$$
  p\left(\theta \mid D, m_{i}\right)=\frac{p\left(D \mid \theta, m_{i}\right) p\left(\theta \mid m_{i}\right)}{p\left(D \mid m_{i}\right)}
$$
which can also be written
$$
  \text { posterior }=\frac{\text { likelihood } \times \text { prior }}{\text { evidence }}
$$
The normalizing constant
$$
  p\left(D \mid m_{i}\right)
$$
is irrelevant to the first level of inference, i.e., the inference of $\theta$;
but it becomes important in the second level of inference, and we name it the
evidence for $m_{i}$.

It is common practice to use gradient-based methods to find the maximum
of the posterior, which defines the most probable value for the parameters,
$\hat{\theta}_{M A P}$; it is then usual to summarize the posterior
distribution by the value of $\hat{\theta}_{M A P}$, and error bars or
confidence intervals on these fitted parameters. Error bars can be obtained from the curvature of the negative
log-posterior. Evaluating the Hessian at $\hat{\theta}_{\text {MAP }}$,
\begin{align}
  A=-\left.\nabla^{2} \ln p\left(\theta \mid D, m_{i}\right)\right|_{\hat{\theta}_{M A P}}
  \label{eq:curvature of the negative
    log-posterior}
\end{align}
we may then see that the posterior can be locally approximated as a Gaussian
with covariance matrix $A^{-1}$. Indeed, Taylor-expanding the log-posterior around the point
$\hat{\theta}_{\text {MAP }}$ and then re-exponentiating gives
$$
  p\left(\theta \mid D, m_{i}\right) \approx p\left(\hat{\theta}_{M A P} \mid D, m_{i}\right) \exp \left(-\frac{1}{2} \Delta \theta^\top A \Delta \theta\right)
  \quad \text { where } \quad  \Delta \theta=\hat{\theta}_{M A P}-\theta
$$

\subsection{The second level of inference}
At the second level of inference, we wish to infer which model is the most
plausible given the data. This entails computing the posterior over models,
\begin{align}
  p(m \mid D)=\frac{p(D \mid m) p(m)}{\sum_{m \in M} p(m, D)}
  \label{eq:posterior over models}
\end{align}
where $D$ is the data.

Given the posterior over models, one can in principle compute the model
with the greatest posterior probability, and also perform pairwise comparisons
of models. The model which is most plausible given the data (the maximum
a-posteriori or MAP model) is computed as
$$
  \hat{m}=\underset{m}{\operatorname{argmax}} ~ p(m \mid D) .
$$
This is called {\color{C3}Bayesian model selection}.

Pairwise comparison of models, say $m_{1}$ and $m_{2}$, is summarized by
the posterior odds
$$
  \frac{p\left(m_{1} \mid D\right)}{p\left(m_{2} \mid D\right)}=\frac{p\left(D \mid m_{1}\right)}{p\left(D \mid m_{2}\right)} \times \frac{p\left(m_{1}\right)}{p\left(m_{2}\right)}
$$
The ratio
$$
  \frac{p\left(D \mid m_{1}\right)}{p\left(D \mid m_{2}\right)}
$$
is called the {\color{C3}Bayes factor}.

Thus the data, through the Bayes factor, updates the prior odds to yield
the posterior odds.

\begin{example}
  As an example, let's consider Bayesian model selection with a uniform
  prior over models,
  $$
    p(m) \propto 1
  $$
  In this special case, MAP model selection amounts to picking the model
  which maximizes
  \begin{align}
    p(D \mid m)=\int p(D \mid \theta) p(\theta \mid m) d \theta
    \label{eq:MAP model selection}
  \end{align}
  This is because the ratio $\frac{p\left(m_{1}\right)}{p\left(m_{2}\right)}$ is $1$ in this special case.
\end{example}

The quantity \cref{eq:MAP model selection} is called the {\color{C3}marginal likelihood}, the {\color{C3}integrated
    likelihood}, or the {\color{C3}evidence for model $m$}. Hence model selection is picking the model which has the most evidence
for it. How to compute (an approximation of) this integral will be discussed
below.

\section{Gaussian approximation}
We now engage in a further discussion of the Gaussian approximation to a
posterior distribution.  One of the main points is that the integral in \cref{eq:MAP model selection}, which represents
the ``normalizing constant'' at the first level of inference,
$$
  p(D \mid m)=\int p(D \mid \theta) p(\theta \mid m) d \theta
$$
is hard to compute.

We will approximate this integral by approximating the posterior
distribution, of which this is the normalizing factor, by a Gaussian
distribution. As this discussion concerns a single model $m$, we will in this section
sometime suppress the model $m$ from the notation. The approximation works as follows: suppose $\theta \in \mathbb{R}^{k}$,
and let
$$
  p(\theta \mid D)=\frac{1}{Z} e^{-E(\theta)}
$$
where $E(\theta)$ is called an {\color{C3}energy function} and is equal to the negative log
of the unnormalized posterior,
$$
  E(\theta)=-\log p(\theta \mid D) - \log Z \quad \text{ with } \quad Z=p(D)
$$
being the normalization constant.

Performing a Taylor series expansion around the mode $\theta^{*}$ (i.e.,
the lowest energy state) we get
$$
  E(\theta) \approx E\left(\theta^{*}\right)+\left(\theta-\theta^{*}\right)^\top \mathbf{g}+\frac{1}{2}\left(\theta-\theta^{*}\right)^\top \mathbf{H}\left(\theta-\theta^{*}\right)
$$
where $\mathbf{g}$ is the gradient and $\mathbf{H}$ is the Hessian of the ``energy function'' evaluated at the mode:
$$
  \mathbf{g}=\left.\nabla E(\theta)\right|_{\theta^{*}} \quad \mathbf{H}=\left.\frac{\partial^{2} E(\theta)}{\partial \theta \partial \theta^\top}\right|_{\theta^{*}} .
$$
Since $\theta^{*}$ is the mode, the gradient is zero there. Hence, defining $\hat{p}(\theta \mid D)$ as the associated normal density,
$$
  \begin{aligned}
    \hat{p}(\theta \mid D) & \approx \frac{1}{Z} e^{-E\left(\theta^{*}\right)} \exp \left[-\frac{1}{2}\left(\theta-\theta^{*}\right)^\top \mathbf{H}\left(\theta-\theta^{*}\right)\right] \xlongequal[\text{ by only the mean and the covaraince}]{\text{ Since the Gaussian distribution is determined }} \mathcal{N}\left(\theta \mid \theta^{*}, \mathbf{H}^{-1}\right)                                                                                                       \\
   \Longrightarrow \frac{1}{Z} e^{-E\left(\theta^{*}\right)} & = \frac{1}{\sqrt{(2\pi)^k|\mathbf{H}|}} \Longrightarrow Z=p(D) =e^{-E\left(\theta^{*}\right)}(2 \pi)^{-k / 2}|\mathbf{H}|^{-1 / 2}
  \end{aligned}
$$
The last line follows from the (well-known) normalization constant of
the multivariate Gaussian. 

A Gaussian approximation is often reasonable in the large-sample
situation, since posteriors often become more ``Gaussian-like'' as the sample
size increases, for reasons analogous to the central limit theorem. The same mathematical approximation method is often used in physics
where it is usually referred to as the saddle point approximation, or
Laplace's method. 

We can use the Gaussian approximation to write the log marginal
likelihood as follows, dropping irrelevant constants:
\begin{align}
  \log p(D) \approx \log p\left(D \mid \theta^{*}\right)+\log p\left(\theta^{*}\right)-\frac{1}{2} \log |\mathbf{H}|
  \label{eq:gaussian approximation for log marginal likelihood}
\end{align}
The penalization terms which are added to $\log p\left(D \mid \theta^{*}\right)$ are a measure of model complexity.

If we have a uniform prior,
$$
  p(\theta) \propto 1,
$$
we can drop the second term, and replace $\theta^{*}$ with the MLE,
$\hat{\theta}$.

For many problems the parameter posterior (as distinguished from the
posterior on model space)
$$
  p\left(\theta \mid D, m_{i}\right) \propto p\left(D \mid \theta, m_{i}\right) p\left(\theta \mid m_{i}\right)
$$
has a strong peak at the most probable parameters $\hat{\theta}_{\text {MAP }}$.

We can thus use Laplace's method to approximate this posterior near its
peak. Taking for simplicity the one-dimensional case, the evidence can be
approximated, using Laplace's method, by the height of the peak of the
integrand
$$
  p\left(D \mid \theta, m_{i}\right) p\left(\theta \mid m_{i}\right)
$$
times its width, $\sigma_{\theta \mid D}$ :
$$
\begin{aligned}
  p\left(D \mid m_{i}\right) &\approx p\left(D \mid \hat{\theta}_{M A P}, m_{i}\right) \times p\left(\hat{\theta}_{M A P} \mid m_{i}\right) \sigma_{\theta \mid D}
  \text{ Evidence } &\approx \text{ Best fit likelihood } \times \text{ Occam factor }
\end{aligned}
$$
The quantity $\sigma_{\theta \mid D}$ is the posterior uncertainty in
$\theta$. Suppose, for simplicity of exposition, that the prior
$$
  p\left(\theta \mid m_{i}\right)
$$
is uniform on some rather large interval of size $\sigma_{w}$, representing the
range of values of $\theta$ that were possible a priori, according to $m_{i}$.

Then
$$
  p\left(\hat{\theta}_{M A P} \mid m_{i}\right)=\frac{1}{\sigma_{w}} 
  \quad \text{ and } \quad 
  \text { Occam factor } :=p\left(\hat{\theta}_{M A P} \mid m_{i}\right) \sigma_{\theta \mid D} = \frac{\sigma_{\theta \mid D}}{\sigma_{w}}
$$
so the Occam factor is equal to the ratio of the posterior accessible volume of
$m_{i}$ 's parameter space to the prior accessible volume, or the factor by
which $m_{i}$ 's hypothesis space collapses when the data arrive.

Intuition: the model $m_{i}$ can be viewed as consisting of a certain
number of exclusive submodels, of which only one survives when the data
arrive.

The Occam factor is the inverse of that number. The logarithm of the Occam factor is a measure of the amount of
information we gain about the model's parameters when the data arrive. A complex model having many parameters, each of which is free to vary
over a large range $\sigma_{w}$, will typically be penalized by a stronger
Occam factor than a simpler model. If the posterior is well approximated by a Gaussian, then the Occam
factor is obtained from the determinant of the corresponding covariance matrix
$$
\begin{aligned}
  p(D \mid m) &\approx p\left(D \mid \hat{\theta}_{M A P}, m\right) \times p\left(\hat{\theta}_{M A P} \mid m\right) \operatorname{det}^{-1 / 2}(A / 2 \pi)\\
  \text{ Evidence } &\approx \text{ Best fit likelihood } \times \text{ Occam factor }
\end{aligned}
$$
where $A=-\left.\nabla^{2} \ln p\left(\theta \mid D,
  m_{i}\right)\right|_{\hat{\theta}_{M A P}}$, is the Hessian as computed above in
\cref{eq:curvature of the negative log-posterior}.

In summary, Bayesian model selection with a uniform prior over models
comes down to finding the model $m$ which maximizes
$$
  p(D \mid m)
$$
This can be viewed as an extension of maximum likelihood model
selection: the evidence is obtained by multiplying the best-fit likelihood by
the Occam factor.

To evaluate the Occam factor we need only the Hessian $A$, if the
Gaussian approximation is good. Thus the Bayesian method of model comparison by evaluating the evidence
is no more computationally demanding than the task of finding for each model
the best-fit parameters and their error bars. 

\subsection{Bayesian Occam's razor effect}

One might think that using $p(D \mid m)$
to select models would always favor the model with the most parameters. Interestingly, this is {\color{C3}false}. It would be true if we were to use $p\left(D \mid \hat{\theta}_{m}\right)$ to select models, where $\hat{\theta}_{m}$ is the MLE or MAP estimate of the
parameters for model $m$, because models with more parameters will fit the data
better, and hence achieve higher likelihood (the probability of the observed data $D$ to appear given our parameters $\hat{\theta}_{m}$). However, if we integrate out the parameters, rather than maximizing
them, we are automatically protected from overfitting: models with more
parameters do not necessarily have higher marginal likelihood. This is called the {\color{C3}Bayesian Occam's razor effect} and was discovered by
Jefferys and Berger (1992). See also Mackay (1995).

\textbf{Intuition:} A way to understand the Bayesian Occam's razor is to note that
probabilities must sum to one. Hence, the sum over all possible data sets gives
$$
  \sum_{D} p(D \mid m)=1
$$
Complex models, which can predict many things, must spread their
probability mass thinly, and hence will not obtain as large a probability for
any given data set as simpler models.

\section{Derivation of the BIC}
We now focus on approximating the third term in \cref{eq:gaussian approximation for log marginal likelihood} which is $\log
  |\mathbf{H}|$. Suppose we have sample size $n$ and $D_{i}$ is the predictor-response
pair in the $i$-th sample.
$$
  \mathbf{H}=\sum_{i=1}^{n} \mathbf{H}_{i} \quad \text { where } \quad \mathbf{H}_{i}=\nabla^{2} \log p\left(D_{i} \mid \theta\right)
$$
Let us approximate each $\mathbf{H}_{i}$ by a fixed matrix
$\hat{\mathbf{H}}$. Then we have
$$
  \log |\mathbf{H}| \approx \log |n \hat{\mathbf{H}}|=\log \left(n^{k}|\hat{\mathbf{H}}|\right)=k \log n+\log |\hat{\mathbf{H}}|
$$
where $k=\operatorname{dim}(\theta)$ and we have assumed $\mathbf{H}$ is
full rank.

We can drop the $\log |\hat{\mathbf{H}}|$ term, since it is independent of $n$,
and thus will get overwhelmed by the likelihood. Putting all the pieces together, the desired approximation is
\begin{align}
  \log p(D) \approx \log p(D \mid \hat{\theta})-\frac{k}{2} \log n
  \label{eq:approximation of log likelihood with the approximation on H}
\end{align}

\begin{example}
  Consider linear regression. The MLE is given by
  $$
    \hat{\theta}=\left(X^\top X\right)^{-1} X^\top y
  $$
  and
  $$
    \hat{\sigma}^{2}=\frac{\mathrm{RSS}}{n} \quad \text { where } \quad \mathrm{RSS}=\sum_{i=1}^{n}\left(y_{i}-\hat{\theta}^{\top} x_{i}\right)^{2} .
  $$
  The corresponding log likelihood is given by
  $$
    \log p(D \mid \hat{\theta})=-\frac{n}{2} \log \left(2 \pi \hat{\sigma}^{2}\right)-\frac{n}{2}
  $$
\end{example}

Hence the approximation \cref{eq:approximation of log likelihood with the approximation on H} gives (dropping constant terms)
$$
  \log p(D) \approx-\frac{n}{2} \log \left(\hat{\sigma}^{2}\right)-\frac{k}{2} \log n
$$
where $k$ is the number of variables in the model.

The negative of this, up to a factor of 2, is often called the {\color{C3}Bayesian
information criterion} and written
$$
  \mathrm{BIC}=n \log \left(\hat{\sigma}^{2}\right)+k \log n
$$
in which case, if used in model selection, the model with the lower
$\mathrm{BIC}$ is to be preferred.

\section{Minimum Description Length}
We first review the theory of coding for data compression, and then
apply it to model selection. If you're curious for a full treatment, the seminal book is Cover and
Thomas (2012). We think of a certain block of data, $z$, as a message that we want to
encode and send to someone else (the "receiver"). We think of our model as a way of encoding the datum, and will choose
the most parsimonious model, that is the shortest code, for the transmission.

Suppose first that the possible messages we might want to transmit are
$$
  z_{1}, z_{2}, \ldots, z_{m}
$$
We want a way of encoding which from among this finite set of possible
messages is actually being transmitted, using some form of binary encoding,
i.e. each message will be mapped somewhat arbitrarily onto a sequence of $1$s and $0$s.

Here is an example with four possible messages:
$$
  \begin{array}{ccccc}
    \text { Message } & z_{1} & z_{2} & z_{3} & z_{4} \\
    \text { Code }    & 0     & 10    & 110   & 111
  \end{array}
$$
This code is known as an instantaneous prefix code - no code is the
prefix of any other, and the receiver (who knows all of the possible codes),
knows exactly when the message has been completely sent. We restrict our discussion to such instantaneous prefix codes. We could permute the codes, for example use codes
$$
  110,10,111,0
$$
for
$$
  z_{1}, z_{2}, z_{3}, z_{4}
$$
The best way to do this depends on how often we will be sending each of
the messages. If, for example, we will be sending $z_{1}$ most often, it makes sense
to use the shortest code 0 for $z_{1}$. Using this kind of strategy-shorter codes for more frequent messages-the
average message length will be shorter. In general, if messages are sent with probabilities $p_{i}$, Shannon's
theorem says we should use code lengths
$$
  \ell_{i}=-\log _{2} p_{i}
$$
and the average message length satisfies
$$
  \mathbb{E}(\text { length }) \geq-\sum_{i} p_{i} \log _{2} p_{i}
$$
The right-hand side above is also called the entropy of the
distribution. In other words: To transmit a random variable $z$ having probability
density function $p(z)$, we require about
$$
  -\log _{2} p(z)
$$
bits of information.

This can be applied to continuous random variables also. With a finite code length we cannot code a continuous variable exactly. However, if we code $z$ within a tolerance $\delta z$, the message
length needed is the log of the probability in the interval
$$
  [z, z+\delta z]
$$
which is well approximated by $\delta z p(z)$ if $\delta z$ is small.

Since
$$
  \log [\delta z p(z)]=\log \delta z+\log p(z)
$$
this means we can just ignore the constant $\log \delta z$ and regard
$$
  \log p(z)
$$
as our measure of message length.

Now we apply this result to the problem of model selection in a
supervised learning context. We have a model $m$ with parameters $\theta$, and data
$$
  Z=(X, y)
$$
consisting of both inputs and outputs.

Let the (conditional) probability of the outputs under the model be
$$
  p(y \mid \theta, m, X)
$$
assume the receiver knows all of the inputs, and we wish to transmit the
outputs.

Then the message length required to transmit the outputs is
$$
  \text { length }=-\log p(y \mid \theta, m, X)-\log p(\theta \mid m)
$$
The second term is the average code length for transmitting the model
parameters $\theta$, while the first term is the average code length for
transmitting the discrepancy between the model and actual target values. The MDL principle says that we should choose the model that minimizes
(12.10). We recognize (12.10) as the (negative) log-posterior distribution, and
hence minimizing description length is equivalent to maximizing posterior
probability. Hence the BIC criterion, derived as approximation to log-posterior
probability, can also be viewed as a device for (approximate) model choice by
minimum description length.

\section{Model Averaging}
Suppose we want to predict a new observation $y$. Let
$$
  D=\left\{y_{1}, \ldots, y_{n}\right\}
$$
be the observed data so far. Then
$$
  p(y \mid D)=\sum_{j} p\left(y \mid D, m_{j}\right) p\left(m_{j} \mid D\right)
$$
where the sum is over the different models being considered. Recalling (12.2), we have
$$
  p\left(m_{j} \mid D\right)=\frac{p\left(D \mid m_{j}\right)}{\sum_{j} p\left(m_{j}, D\right)}
$$
when we have a uniform prior over models.

Moreover, according to (12.8), the log of the numerator of the last
equation becomes
$$
  \log p\left(D \mid m_{j}\right) \approx \log p\left(D \mid \hat{\theta}, m_{j}\right)-\frac{k_{j}}{2} \log n \equiv-2 \cdot \mathrm{BIC}_{j}
$$
This means we can approximate
$$
  p\left(m_{j} \mid D\right)
$$
by
$$
  \exp \left(-2 \cdot \mathrm{BIC}_{j}\right) /\left(\sum_{j} \exp \left(-2 \cdot \mathrm{BIC}_{j}\right)\right)
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{thebibliography}{99}

  \bibitem{cover2012}
  Thomas M. Cover and Joy A. Thomas.
  \newblock \emph{Elements of Information Theory}.
  \newblock John Wiley \& Sons, 2012.

  \bibitem{jefferys1992}
  William H. Jefferys and James O. Berger.
  \newblock Ockham's razor and Bayesian analysis.
  \newblock In: \emph{American Scientist}, Vol. 80.1, 1992, pp. 64-72.

  \bibitem{mackay1995}
  David J.C. MacKay.
  \newblock Probable networks and plausible predictions - a review of practical Bayesian methods for supervised neural networks.
  \newblock In: \emph{Network: Computation in Neural Systems}, Vol. 6.3, 1995, pp. 469-505.

\end{thebibliography}






























\end{document}

